{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TPf8CY_Xumr"
      },
      "source": [
        "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
        "     width=\"200px\"\n",
        "     height=\"auto\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92kCWnfbXumv"
      },
      "source": [
        "# ‚õΩ Fuel efficiency Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "Provided with the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset, we will predict the **fuel efficiency** of the late-1970s and early 1980s automobiles, leveraging features such as cylinders, displacement, horsepower, weight, etc. \n",
        "\n",
        "It is a very small dataset and there are only a few features. We will first build a linear model and a neural network, evaluate their performances, track our experiment runs and inspect the logs using MLflow, and apply [TPOT](https://github.com/EpistasisLab/tpot) to see how it can be used to search over many ML model architectures, followed by explaining the model with SHAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxVwvxwii96J"
      },
      "source": [
        "# üìö Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YCC4uUnXumw"
      },
      "source": [
        "By the end of this session, you will be able to\n",
        "\n",
        "- understand the core building blocks of a neural network\n",
        "- understand what dense and activation layers do\n",
        "- build, train, and evaluate neural networks\n",
        "- track tensorflow experiments with MLflow, access information of runs programmatically and with its tracking ui\n",
        "- perform AutoML to search for optimal tree-based pipeline for a regression task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7PFU9Oza7-O"
      },
      "source": [
        "Note: [State of Data Science and Machine Learning 2021](https://www.kaggle.com/kaggle-survey-2021) by Kaggle shows that the most commonly used algorithms were linear and logtistic regressions, followed closely by decision trees, random forests, and gradient boosting machines (are you surprised?). Multilayer perceptron, or artificial neural networks are not yet the popular tools for tabular/structured data; see more technical reasons in papers: [Deep Neural Networks and Tabular Data: A Survey](https://arxiv.org/abs/2110.01889), [Tabular Data: Deep Learning is Not All You Need](https://arxiv.org/abs/2106.03253). For this assignment, the main purpose is for you to get familiar with the basic building blocks in constructing neural networks before we dive into more specialized neural network architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n584Aq-tmV4"
      },
      "source": [
        "**IMPORTANT**\n",
        "\n",
        "You only need to run the following cells if you're completing the assignment in Google Collab. If you've already installed these libraries locally, you can skip installing these libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjvDr0LBtmV4",
        "outputId": "3c91ffee-3387-49a9-a0d4-7e1af4f88950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect colab to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moB4tpEHxKB3",
        "outputId": "df0490e2-09f7-416a-eba9-39c1da6bb1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m383.2/383.2 KB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m174.5/174.5 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.4/85.4 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q seaborn # pairplot\n",
        "!pip install -q tpot  # automl\n",
        "\n",
        "!pip install -q mlflow==1.11.0 # tracking\n",
        "!pip install -q pyngrok # workaround to run mlflow ui in colab\n",
        "!pip install -q shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xQKvCJ85kCQ",
        "outputId": "ab704f70-66f8-4a70-f7da-575694e6ae44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Task 1 - Data: Auto MPG dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvK47Su6t0I8"
      },
      "source": [
        "0. Start MLflow's automatic logging using library-specific autolog calls for tensorflow: logging metrics, parameters, and models without the need for explicit log statements. \n",
        "    \n",
        "    We will get into more details using **MLflow** after completing our experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PGEnlTVlFDK"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "mlflow.tensorflow.autolog() # MLflow Autologging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "1. The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/). First download and import the dataset using `pandas`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = [\n",
        "  'MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "  'Acceleration', 'Model Year', 'Origin'\n",
        "  ]\n",
        "\n",
        "dataset = pd.read_csv(url, names=column_names, na_values='?', \n",
        "                      comment='\\t', sep=' ', skipinitialspace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oY3pMPagJrO"
      },
      "outputs": [],
      "source": [
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "2. The dataset contains a few unknown values, we drop those rows to keep this initial tutorial simple. Use `pd.DataFrame.dropna()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZUDosChC1UN"
      },
      "outputs": [],
      "source": [
        "dataset = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKitwaH4v8h"
      },
      "source": [
        "3. The `\"Origin\"` column is categorical, not numeric. So the next step is to one-hot encode the values in the column with [pd.get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWNTD2QjBWFJ"
      },
      "outputs": [],
      "source": [
        "dataset['Origin'] = dataset['Origin'].replace({1: 'USA', 2: 'Europe', 3: 'Japan'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulXz4J7PAUzk"
      },
      "outputs": [],
      "source": [
        "dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuym4yvk76vU"
      },
      "source": [
        "4. Split the data into training and test sets. To reduce the module importing overhead, instead of `sklearn.model_selection.train_test_split()`, use `pd.DataFrame.sample()` to save 80% of the data aside to `train_dataset`, set the random state to be 0 for reproducibility. \n",
        "\n",
        "   Then use `pd.DataFrame.drop()` to obtain the `test_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn-IGhUE7_1H"
      },
      "outputs": [],
      "source": [
        "train_dataset = None # YOUR CODE HERE\n",
        "test_dataset = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ubs136WLNp"
      },
      "source": [
        "5. Review the pairwise relationships of a few pairs of columns from the training set. \n",
        "   \n",
        "   The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKO_x8gWKv-"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gavKO_6DWRMP"
      },
      "source": [
        "Let's also check the overall statistics. Note how each feature covers a very different range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "outputs": [],
      "source": [
        "train_dataset.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7Auq1yXUvh"
      },
      "source": [
        "6. Split features from labels. \n",
        "  This means, separate the target value(also called\"label\") from the features. \n",
        "  Label is the value that you will train the model to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2sluJdCW7jN"
      },
      "outputs": [],
      "source": [
        "train_features = None # YOUR CODE HERE\n",
        "test_features = None # YOUR CODE HERE\n",
        "\n",
        "train_labels = None # YOUR CODE HERE\n",
        "test_labels = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRklxK5s388r"
      },
      "source": [
        "# Task 2 - Normalization Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ywmerQ6dSox"
      },
      "source": [
        "It is good practice to normalize features that use different scales and ranges. Although a model *might* converge without feature normalization, normalization makes training much more stable.\n",
        "\n",
        "Similar to scikit-learn, tensorflow.keras offers a list of [preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) so that you can build and export models that are truly end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJ6ISropeoo"
      },
      "source": [
        "1. The Normalization layer ([`tf.keras.layers.Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) is a clean and simple way to add feature normalization into your model. The first step is to create the layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlC5ooJrgjQF"
      },
      "outputs": [],
      "source": [
        "normalizer = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYA2Ap6nVOha"
      },
      "source": [
        "2. Then, fit the state of the preprocessing layer to the data by calling [`Normalization.adapt`](https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrBbbjbwV91f"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqryjQ_DMMHp"
      },
      "source": [
        "We can see the feature mean and variance are stored in the layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lreV_KOML142"
      },
      "outputs": [],
      "source": [
        "print(f'feature mean: {normalizer.mean.numpy().squeeze()}\\n')\n",
        "print(f'feature variance: {normalizer.variance.numpy().squeeze()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWKaF9GSRuN"
      },
      "source": [
        "When the layer is called, it returns the input data, with each feature independently normalized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l7zFL_XWIRu"
      },
      "outputs": [],
      "source": [
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "    print('First example:', first)\n",
        "    print()\n",
        "    print('Normalized:', normalizer(first).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "# Task 3 - Linear Regression üìà\n",
        "\n",
        "Before building a deep neural network model, start with linear regression using all the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFby9n0tnHkw"
      },
      "source": [
        "Training a model with `tf.keras` typically starts by defining the model architecture. Use a `tf.keras.Sequential` model, which [represents a sequence of steps](https://www.tensorflow.org/guide/keras/sequential_model).\n",
        "\n",
        "There are two steps in this multivariate linear regression model:\n",
        "\n",
        "- Normalize all the input features using the `tf.keras.layers.Normalization` preprocessing layer. You have defined this earlier as `normalizer`.\n",
        "- Apply a linear transformation ($y = mx+b$ where $m$ is a matrix and $b$ is a vector.) to produce one output using a linear layer ([`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)).\n",
        "\n",
        "The number of _inputs_ can either be set by the `input_shape` argument, or automatically when the model is run for the first time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NVlHJY2TWlC"
      },
      "source": [
        "1. Build the Keras Sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "outputs": [],
      "source": [
        "linear_model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE\n",
        "    # YOUR CODE HERE\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iaY0wviy0FO"
      },
      "outputs": [],
      "source": [
        "linear_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObQu9fDnXGL"
      },
      "source": [
        "2. This model will predict `'MPG'` from all features in `train_features`. Run the untrained model on the first 10 data points / rows using `Model.predict()`. The output won't be good, but notice that it has the expected shape of `(10, 1)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfV1HS6bns-s"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFND0vL4y5OZ"
      },
      "source": [
        "3. When you call the model, its weight matrices will be built‚Äîcheck that the `kernel` weights (the $m$ in $y = mx + b$) have a shape of (9, 1):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaqBYReyzBgr"
      },
      "outputs": [],
      "source": [
        "linear_model.layers[1].kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkanJlmmFBX"
      },
      "source": [
        "4. Once the model is built, configure the training procedure using the Keras `Model.compile` method. The most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized and how (using the `tf.keras.optimizers.Adam`).\n",
        "\n",
        "  Here's a list of built-in loss functions in [`tf.keras.losses`](https://www.tensorflow.org/api_docs/python/tf/keras/losses). For regression tasks, [common loss functions](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3) include mean squared error (MSE) and mean absolute error (MAE). Here,  MAE is preferred such that the model is more robust against outliers. \n",
        "\n",
        "  For optimizers, gradient descent (check this video [Gradient Descent, Step-by-Step](https://www.youtube.com/watch?v=sDv4f4s2SB8) for a refresher) is the preferred way to optimize neural networks and many other machine learning algorithms. Read [an overview of graident descent optimizer algorithms](https://ruder.io/optimizing-gradient-descent/) for several popular gradient descent algorithms. Here, we use the popular [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam), and set the learning rate at 0.1 for faster learning.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxA_3lpOm-SK"
      },
      "outputs": [],
      "source": [
        "linear_model.compile(\n",
        "    optimizer=None, # YOUR CODE HERE\n",
        "    loss=None # YOUR CODE HERE\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3q1I9TwnRSC"
      },
      "source": [
        "5. Use Keras `Model.fit` to execute the training for 100 epochs, set the verbose to 0 to suppress logging and keep 20% of the data for validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iSrNy59nRAp"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQm3pc0FYPQB"
      },
      "source": [
        "6. Visualize the model's training progress using the stats stored in the `history` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCAwD_y4AdC3"
      },
      "outputs": [],
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E54UoZunqhc"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.ylim([0, 10])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error [MPG]')\n",
        "    plt.legend()\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk81ixIgFcEU"
      },
      "source": [
        "Use `plot_loss(history)` provided to visualize the progression in loss function for training and validation data sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYsQYrIZyqjz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMNrt8X2ebXd"
      },
      "source": [
        "7. Collect the results on the test set for later using [`Model.evaluate()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDZ8EvNYrDtx"
      },
      "outputs": [],
      "source": [
        "test_results = {}\n",
        "\n",
        "test_results['linear_model'] = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh0trI_zGCT1"
      },
      "outputs": [],
      "source": [
        "test_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjdzxKzEu1-"
      },
      "source": [
        "# Task 4 - Regression with a Deep Neural Network (DNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_aHPsrzO1t"
      },
      "source": [
        "You just implemented a linear model for multiple inputs. Now, you are ready to implement multiple-input DNN models.\n",
        "\n",
        "The code is very similar except the model is expanded to include some \"hidden\" **non-linear** layers. The name \"hidden\" here just means not directly connected to the inputs or outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWtkIjhrZwa"
      },
      "source": [
        "* The normalization layer, as before (with `normalizer` for a multiple-input model).\n",
        "* Two hidden, non-linear, [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with the ReLU (`relu`) activation function nonlinearity. One way is to set parameter `activation` inside `Dense` Set the number of neurons at each layer to be 64. \n",
        "* A linear `Dense` single-output layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6btF_xJPpW"
      },
      "source": [
        "\n",
        "\n",
        "1. Include the model and `compile` method in the `build_and_compile_model` function below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c26juK7ZG8j-"
      },
      "outputs": [],
      "source": [
        "def build_and_compile_model(norm):\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvu9gtxTZR5V"
      },
      "source": [
        "2. Create a DNN model with `normalizer` (defined earlier) as the normalization layer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGbPb-PHGbhs"
      },
      "outputs": [],
      "source": [
        "dnn_model = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj49Og4YGULr"
      },
      "source": [
        "3. Inspect the model using `Model.summary()`. This model has quite a few more trainable parameters than the linear models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReAD0n6MsFK-"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qWCsh6DlyH"
      },
      "source": [
        "4. Train the model with Keras `Model.fit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD7qHCmNIOY0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dArGGxHxcKjN"
      },
      "source": [
        "5. Visualize the model's training progress using the stats stored in the history object. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcF6UWjdCU8T"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRIDHn59LTax"
      },
      "source": [
        "Do you think the DNN model is overfitting? What gives away? \n",
        "\n",
        "    *YOUR ANSWER HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dhMN7IUM12Q"
      },
      "source": [
        "6. Let's save the results for later comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJjM0dU52XtN"
      },
      "outputs": [],
      "source": [
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiCucdPLfMkZ"
      },
      "source": [
        "# Task 5 - Make Predictions üîÆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDf1xebEfWBw"
      },
      "source": [
        "1. Since both models have been trained, we can review their test set performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5_ooufM5iH2"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DABIVzsCf-QI"
      },
      "source": [
        "These results match the validation error observed during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft603OzXuEZC"
      },
      "source": [
        "2. We can now make predictions with the `dnn_model` on the test set using Keras `Model.predict` and review the loss. Use `.flatten()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "outputs": [],
      "source": [
        "test_predictions = None # YOUR CODE HERE\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19wyogbOSU5t"
      },
      "source": [
        "3. It appears that the model predicts reasonably well. Now, check the error distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-OHX4DiXd8x"
      },
      "outputs": [],
      "source": [
        "error = None # YOUR CODE HERE\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "_ = plt.ylabel('Count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSyaHUfDT-mZ"
      },
      "source": [
        "4. Save it for later use with `Model.save`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-WwLlmfT-mb"
      },
      "outputs": [],
      "source": [
        "dnn_model.save('dnn_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Benlnl8UT-me"
      },
      "source": [
        "5. Reload the model with `Model.load_model`; it gives identical output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyyyj2zVT-mf"
      },
      "outputs": [],
      "source": [
        "reloaded = None # YOUR CODE HERE\n",
        "\n",
        "test_results['reloaded'] = reloaded.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_GchJ2tg-2o"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKeDqQVlgEvL"
      },
      "source": [
        "# Task 6 - Nonlinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi4qB9oDPU4j"
      },
      "source": [
        "We mentioned that the `relu` activation function introduce non-linearity; let's visualize it. Since there are six numerical features and 1 categorical features, it is impossible to plot all the dimensions on a 2D plot; we need to simplify/isolate it. \n",
        "\n",
        "Note: in this task, code is provided; the focus in on understanding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WZj2vEGQwOL"
      },
      "source": [
        "1. We focus on the relationship between feature `Displacement` and target `MPG`. \n",
        "  \n",
        "  To do so, create a new dataset of the same size as `train_features`, but all other features are set at their median values; then set the `Displacement` between 0 and 500. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXcntGZv-CeV"
      },
      "outputs": [],
      "source": [
        "fake = np.outer(np.ones(train_features.shape[0]), train_features.median())\n",
        "fake = pd.DataFrame(fake, columns = train_features.columns)\n",
        "fake.Displacement = np.linspace(0, 500, train_features.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwYXI7n1VIF3"
      },
      "source": [
        "2. Create a plotting function to:\n",
        "\n",
        "    a) visualize real values between `Displacement` and `MPG` from the training dataset in scatter plot \n",
        "    \n",
        "    b) overlay the predicted MPG from Displacement varying from 0 to 500, but holding all other features constant. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXDFyU4v-mak"
      },
      "outputs": [],
      "source": [
        "def plot_displacement(x, y):\n",
        "    plt.scatter(train_features['Displacement'], train_labels, label='Data')\n",
        "    plt.plot(x, y, color='k', label='Predictions')\n",
        "    plt.xlabel('Displacement')\n",
        "    plt.ylabel('MPG')\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLXZ7QsmVl_i"
      },
      "source": [
        "3. Visualize predicted MPG using the linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0M1mhutB9YQ"
      },
      "outputs": [],
      "source": [
        "plot_displacement(fake.Displacement, linear_model(fake))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Iw161_VvsP"
      },
      "source": [
        "4. Visualize predicted MPG using the neural network model. Do you see an improvement/non-linearity from the linear model? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXe9lxlvBWbk"
      },
      "outputs": [],
      "source": [
        "plot_displacement(fake.Displacement, dnn_model.predict(fake))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXWsMH5gR5pE"
      },
      "source": [
        "5. What are the other activation functions? Check the list of [activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations). \n",
        "  \n",
        "  Optional. Modify the DNN model with a different activation function, and fit it on the data; does it perform better? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "6. Overfitting is a common problem for DNN models, how should we deal with it? Check [Regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) on tf.keras. Any other techiniques that are invented for neural networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP8i8p3HrfdN"
      },
      "source": [
        "# Task 7 - MLflow Tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saYwy3qTv7Sm"
      },
      "source": [
        "In this task, we briefly explore [MLflow Tracking](https://www.mlflow.org/docs/latest/tracking.html#tracking), one of four primary functions that MLflow offers for managing the end-to-end machine learning lifecycle. We will access the information runs programmatically in python and then set up the MLflow UI for easy interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85HHzfclv6hp"
      },
      "source": [
        "1. Experiments.\n",
        "\n",
        "    MLflow Tracking is organized around the concept of `runs`, which are executions of some piece of modeling code; and runs are organized into experiments. \n",
        "\n",
        "    We set the auto logging in the beginning, we can verify that\n",
        "    - there is one experiment\n",
        "    - its name is `0`\n",
        "    - all of its artifacts are stored at `file:///content/mlruns/0` in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PetDDBPzrpNN"
      },
      "outputs": [],
      "source": [
        "from mlflow import MlflowClient\n",
        "client = MlflowClient()\n",
        "client.list_experiments() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU5LVdyi8W1F"
      },
      "source": [
        "2. Runs. \n",
        "\n",
        "   List information for runs that are under experiment '0' using [`mlflow.list_run_infos()`](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.list_run_infos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjHYHCZSpd_Y"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74ail7H8W1F"
      },
      "source": [
        "3. Retrieve the currently active run, i.e., the DNN model. Hint: `mlflow.last_active_run()` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25LDySIXetDJ"
      },
      "outputs": [],
      "source": [
        "autolog_run = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNbi3mfZ8W1G"
      },
      "source": [
        "4. Use function `print_auto_logged_info` provided below to fetch the auto logged parameters and metrics for `autolog_run`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          1
        ],
        "id": "tejN3IaiqzNn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def print_auto_logged_info(r):\n",
        "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
        "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
        "    print(\"run_id: {}\".format(r.info.run_id))\n",
        "    print(\"artifacts: {}\".format(artifacts))\n",
        "    print(\"params: {}\".format(json.dumps(r.data.params, indent=4)))\n",
        "    print(\"metrics: {}\".format(r.data.metrics))\n",
        "    print(\"tags: {}\".format(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbo72KK3q1KS"
      },
      "outputs": [],
      "source": [
        "print_auto_logged_info(\n",
        "    # YOUR CODE HERE\n",
        "    ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BeuP-WP8W1G"
      },
      "source": [
        "5. Optional. Retrieve the best run using [MlflowClient().search_runs()](https://www.mlflow.org/docs/latest/search-runs.html#python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmIpgSJbsFD8"
      },
      "outputs": [],
      "source": [
        "runs = None # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh1zQTm3s0P5"
      },
      "outputs": [],
      "source": [
        "runs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-EgegmM8W1G"
      },
      "source": [
        "6. To see what's logged in the file system `/content/mlruns/`, click tab `files` in the left sidepanel in Colab. For example, \n",
        "    ```\n",
        "    mlruns\n",
        "    ‚îî‚îÄ‚îÄ 0\n",
        "        ‚îú‚îÄ‚îÄ 3a5aebdd35ef46fb8dc35b40e542f0a4\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ artifacts\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ meta.yaml\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ metrics\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ params\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ tags\n",
        "        ‚îú‚îÄ‚îÄ c627bc526c4a4c418a8285627e61a16d\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ artifacts\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ meta.yaml\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ metrics\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ params\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ tags\n",
        "        ‚îî‚îÄ‚îÄ meta.yaml\n",
        "\n",
        "    11 directories, 3 files\n",
        "    ```\n",
        "\n",
        "    Inspect the model summary of the DNN model you ran previously; it is located at `artifacts/model_summary.txt` of the corresponding run. Use `cat $filepath`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MQfSU1guWB5"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (bash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZp8rnL0pBnh"
      },
      "source": [
        "7. Tracking UI. \n",
        "\n",
        "    MLflow provides an UI for us to visualize, search and compare runs, as well as download run artifacts or metadata for analysis in other tools. \n",
        "\n",
        "    If your runs are logged to a local mlruns directory, run `mlflow ui` in the directory above it will load the corresponding runs. \n",
        "    \n",
        "    Running localhost server in Colab, however, requires a bit of extra work:\n",
        "    - set up a free account on [ngrok](https://dashboard.ngrok.com/get-started/setup)\n",
        "    - retrieve the authtoken from https://dashboard.ngrok.com/auth and update the code cell below\n",
        "    \n",
        "   **NOTE**. NEVER share your secrets. Best to keep `NGROK_AUTH_TOKEN` as an environment variable and retrieve it via `os.environ.get(\"NGROK_AUTH_TOKEN\")`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0M6y71efq1I"
      },
      "outputs": [],
      "source": [
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\") \n",
        "\n",
        "# create remote tunnel using ngrok.com to allow local port access\n",
        "from pyngrok import ngrok\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (see Note above)\n",
        "NGROK_AUTH_TOKEN = \"2EpLAwBPBxY5nlcuUvddGKOumUD_7pfLCJRhe9ku896xysV46\"  # YOUR CODE HERE\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_kE6F5t8W1H"
      },
      "source": [
        "8. Interact with Tracking UI. \n",
        "\n",
        "    Open the link, output from the previous cell. get oriented, `Parameters`, `Metrics`, `Artifacts`, and so on.\n",
        "    \n",
        "    When you are done, make sure to terminate the open tunnel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0aY2xWOFAvM"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06DnCpbACGNM"
      },
      "source": [
        "# Task 8 - AutoML with TPOT ü´ñ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djIwpag8DsEu"
      },
      "source": [
        "1. Instantiate and train a TPOT auto-ML regressor.\n",
        "\n",
        "  The parameters are set fairly arbitrarily (if time permits, you shall experiment with different sets of parameters after reading [what each parameter does](http://epistasislab.github.io/tpot/api/#regression)). Use these parameter values:\n",
        "\n",
        "  `generations`: 10\n",
        "\n",
        "  `population_size`: 40\n",
        "\n",
        "  `scoring`: negative mean absolute error; read more in [scoring functions in TPOT](http://epistasislab.github.io/tpot/using/#scoring-functions)\n",
        "\n",
        "  `verbosity`: 2 (so you can see each generation's performance)\n",
        "\n",
        "  The final line with create a Python script tpot_products_pipeline.py with the code to create the optimal model found by TPOT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvJpNXHg_RS5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from tpot import TPOTRegressor\n",
        "tpot = TPOTRegressor(generations=10, \n",
        "                     population_size=40,\n",
        "                     scoring=None, # YOUR CODE HERE\n",
        "                     verbosity=2,\n",
        "                     random_state=42)\n",
        "tpot.fit(train_features, train_labels)\n",
        "print(f\"Tpop score on test data: {tpot.score(test_features, test_labels):.2f}\")\n",
        "tpot.export('tpot_mpg_pipeline.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ne0SBfZIwsF"
      },
      "source": [
        "2. Examine the model pipeline that TPOT regressor offers. If you see any model, function, or class that are not familiar, look them up! \n",
        "\n",
        "  Note: There is randomness to the way the TPOT searches, so it's possible you won't have exactly the same result as your classmate. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LdJz0Bfg7Sg"
      },
      "outputs": [],
      "source": [
        "cat tpot_mpg_pipeline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNsj_X6zJzb5"
      },
      "source": [
        "3. Take the appropriate lines (e.g., updating path to data and the variable names) from `tpot_mpg_pipeline.py` to build a model on our training set and make predictions on the test set. \n",
        "Save the predictions as `y_pred`, and compute appropriate evaluation metric. \n",
        "You may find that for this simple data set, the neural network we built outperforms the tree-based model, yet note it is not a conclusion that we can generalize for all tabular data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUEekD-ZtmWL"
      },
      "source": [
        "# Task 9 - Model Explainability\n",
        "\n",
        "Last week, we introduced model explainability with SHAP and will continue to incorporate it as part our model output this week.  You can use the [Kernel Explainer](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/neural_networks/Census%20income%20classification%20with%20Keras.html) for explainability of both the Neural Networks and the TPOT classifier.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80mtSb2mtmWL"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "# YOUR CODE GOES HERE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ec-ZXUntmWL"
      },
      "source": [
        "# Task 10 - Taking it to the Next Level! üì∂\n",
        "\n",
        "Let's take our models and make a model comparison demo like we did last week, but this time you're taking the lead!  \n",
        "    \n",
        "1. Save your training dataset as a CSV file so that it can be used in the Streamlit app.\n",
        "1. Build a results DataFrame and save it as a CSV so that it can be used in the Streamlit app.\n",
        "1. In Tab 1 - Raw Data:\n",
        "* Display your training dataset in a Streamlit DataFrame (`st.DataFrame`).\n",
        "* Build 1-2 interactive Plotly visualizations that explore the dataset (correlations, scatterplot, etc.)\n",
        "2. In Tab 2 - Model Results:\n",
        "* Display your performance metrics appropriately using 2-3 metrics for model comparison.\n",
        "3. In Tab 3 - Model Explainability:\n",
        "* Make local and global explainability plots to compare two models at a time side-by-side.  [Here](https://www.kaggle.com/code/elsa155026/uciheart-kernel-shap-and-interactive-visualization/notebook) is a good example if how to create some different explainability plots using Plotly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDZ_9p6L_U9L"
      },
      "source": [
        "# Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dtjRavkXunK"
      },
      "source": [
        "- [Tensorflow playground](https://playground.tensorflow.org/) for an interactive experience to understand how nueral networkds work.\n",
        "\n",
        "- [An Introduction to Deep Learning for Tabular Data](https://www.fast.ai/2018/04/29/categorical-embeddings/) covers embeddings for categorical variables. \n",
        "\n",
        "- [Imbalanced classification: credit card fraud detection](https://keras.io/examples/structured_data/imbalanced_classification/) demonstrates using `class_weight` to handle imbalanced classification problems. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "RTRCN3xsXunK"
      },
      "source": [
        "# Acknowledgement and Copyright"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "riVFgcSTmY0R"
      },
      "source": [
        "##### Acknowledgement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "higVWuZMmc7w"
      },
      "source": [
        "This notebook is adapted from [tensorflow/keras tuorial - regression](https://www.tensorflow.org/tutorials/keras/regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "MJuKft5yXunK"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "form",
        "hidden": true,
        "id": "AwOEIRJC6Une"
      },
      "source": [
        "@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "form",
        "hidden": true,
        "id": "KyPEtTqk6VdG"
      },
      "source": [
        "@title MIT License\n",
        "\n",
        "Copyright (c) 2017 Fran√ßois Chollet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a\n",
        "copy of this software and associated documentation files (the \"Software\"),\n",
        "to deal in the Software without restriction, including without limitation\n",
        "the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "and/or sell copies of the Software, and to permit persons to whom the\n",
        "Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "DEALINGS IN THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "244.390625px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}